{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjVUGAooK87a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# UJ Advisor - Chunk Evaluation Pipeline (Cleaned)\n",
        "# NOTE: Removed all OpenAI stuff we don't need anymore\n",
        "\n",
        "\n",
        "!pip install chromadb sentence-transformers tqdm numpy\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "\n",
        "# These are my cleaned JSON files containing all chunks\n",
        "# If a file here loads 0  means the format is wrong\n",
        "RAW_FILES = [\n",
        "    \"/content/CS_normalized.json\",\n",
        "    \"/content/SWE_normalized.json\",\n",
        "    \"/content/STUDENT_RULES.json\",\n",
        "    \"/content/AI_normalized.json\",\n",
        "    \"/content/DS_normalized.json\",\n",
        "    \"/content/CNE_normalized.json\",\n",
        "    \"/content/CY_normalized.json\",\n",
        "]\n",
        "\n",
        "\n",
        "# Helper function:\n",
        "# Reads any JSON and extracts meaningful text chunks from it\n",
        "# This handles: rules, plans, nested fields\n",
        "# this part handles any weird formats\n",
        "\n",
        "def extract_chunks_from_file(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    # 1) Simple rule list files\n",
        "    if isinstance(data, list):\n",
        "        for item in data:\n",
        "            if isinstance(item, dict) and \"text\" in item and isinstance(item[\"text\"], str):\n",
        "                chunks.append(item)\n",
        "\n",
        "    # 2) Study plan files (program ‚Üí levels ‚Üí courses)\n",
        "    if isinstance(data, dict):\n",
        "        program = data.get(\"program\")\n",
        "        levels = data.get(\"levels\", [])\n",
        "\n",
        "        if isinstance(levels, list):\n",
        "            for lvl in levels:\n",
        "                level_id = lvl.get(\"level_id\") or lvl.get(\"level\")\n",
        "                courses = lvl.get(\"courses\", [])\n",
        "                for c in courses:\n",
        "                    text = f\"{c.get('code','')} - {c.get('name','')} | Credits: {c.get('credits',0)}\"\n",
        "                    chunks.append({\n",
        "                        \"text\": text,\n",
        "                        \"metadata\": {\n",
        "                            \"program\": program,\n",
        "                            \"level\": level_id,\n",
        "                            \"dept\": c.get(\"dept\",\"\"),\n",
        "                            \"code\": c.get(\"code\",\"\"),\n",
        "                            \"credits\": c.get(\"credits\",0),\n",
        "                            \"prerequisites\": c.get(\"prerequisites\", [])\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "    # 3) Electives (same logic, different structure)\n",
        "    if isinstance(data, dict) and \"electiveList\" in data:\n",
        "        for item in data[\"electiveList\"]:\n",
        "            text = f\"{item.get('course_code','')} - {item.get('course_name','')}\"\n",
        "            chunks.append({\n",
        "                \"text\": text,\n",
        "                \"metadata\": {\n",
        "                    \"category\": item.get(\"category\",\"\"),\n",
        "                    \"credits\": item.get(\"credits\", 0),\n",
        "                    \"prerequisites\": item.get(\"prerequisites\", [])\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # 4) Deep fallback scan incase of any more weird format\n",
        "    def deep_scan(obj):\n",
        "        if isinstance(obj, dict):\n",
        "            for key in [\"text\", \"description\", \"rule\", \"name\", \"title\"]:\n",
        "                if key in obj and isinstance(obj[key], str):\n",
        "                    chunks.append({\"text\": obj[key], \"metadata\": {}})\n",
        "            for v in obj.values():\n",
        "                deep_scan(v)\n",
        "\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                deep_scan(item)\n",
        "\n",
        "    deep_scan(data)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Load all the chunks from every file\n",
        "\n",
        "all_chunks = []\n",
        "for file in RAW_FILES:\n",
        "    if not os.path.exists(file):\n",
        "        print(f\"File not found: {file}\")\n",
        "        continue\n",
        "\n",
        "    extracted = extract_chunks_from_file(file)\n",
        "    print(f\"üì• {file}: {len(extracted)} chunks loaded\")\n",
        "    all_chunks.extend(extracted)\n",
        "\n",
        "print(\"TOTAL CHUNKS LOADED:\", len(all_chunks))\n",
        "\n",
        "\n",
        "# Load multilingual-E5-large ‚Üí Our embedding workhorse\n",
        "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
        "\n",
        "# Initialize ChromaDB (persistent folder)\n",
        "from chromadb import PersistentClient\n",
        "chroma = PersistentClient(path=\"/content/chroma_store\")\n",
        "collection = chroma.get_or_create_collection(\"rag_collection\")\n",
        "\n",
        "# Prepare all components (texts, ids, metadata)\n",
        "\n",
        "texts = [c[\"text\"] for c in all_chunks]\n",
        "\n",
        "ids = [c.get(\"id\", f\"chunk_{i}\") for i in range(len(all_chunks))]\n",
        "\n",
        "# Metadata cleanup to fit chromaDB needs\n",
        "def sanitize_metadata(md):\n",
        "    clean = {}\n",
        "    for k, v in md.items():\n",
        "        if isinstance(v, list):\n",
        "            clean[k] = \", \".join(str(x) for x in v)\n",
        "        elif isinstance(v, dict):\n",
        "            clean[k] = json.dumps(v, ensure_ascii=False)\n",
        "        elif v is None:\n",
        "            clean[k] = \"\"\n",
        "        else:\n",
        "            clean[k] = v\n",
        "\n",
        "    if len(clean) == 0:\n",
        "        clean[\"source\"] = \"chunk\"\n",
        "\n",
        "    return clean\n",
        "\n",
        "metadatas = [sanitize_metadata(c.get(\"metadata\", {})) for c in all_chunks]\n",
        "\n",
        "\n",
        "# Embed all chunks\n",
        "\n",
        "print(\"Embedding\", len(texts), \"chunks‚Ä¶\")\n",
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "collection.add(\n",
        "    embeddings=embeddings.tolist(),\n",
        "    documents=texts,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(\" ChromaDB Collection Ready\")\n",
        "\n",
        "\n",
        "# Simple retrieve()  returns top-k relevant chunks\n",
        "\n",
        "def retrieve(query, k=5):\n",
        "    q_emb = model.encode([query])[0].tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=[q_emb],\n",
        "        n_results=k\n",
        "    )\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  semantic scoring + keyword scoring.\n",
        "\n",
        "from sentence_transformers import util\n",
        "import torch\n",
        "\n",
        "def embed(text):\n",
        "    return model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "# Semantic similarity using cosine similarity\n",
        "def semantic_score(query, retrieved, ground_truth):\n",
        "\n",
        "    q = embed(query)\n",
        "    r = embed(retrieved)\n",
        "    gt = embed(ground_truth)\n",
        "\n",
        "    sim1 = util.cos_sim(q, r).item()  # how close retrieval is to query\n",
        "    sim2 = util.cos_sim(r, gt).item() # how close retrieval is to ground truth\n",
        "\n",
        "    return (sim1 + sim2) / 2\n",
        "\n",
        "# Simple keyword overlap measure\n",
        "def keyword_score(retrieved, keywords):\n",
        "    if not keywords:\n",
        "        return 0\n",
        "    return sum(kw.strip() in retrieved for kw in keywords) / len(keywords)\n",
        "\n",
        "# Final hybrid score: semantic (80%) + keywords (20%)\n",
        "def hybrid_score(query, retrieved, keywords, ground_truth):\n",
        "\n",
        "    sem = semantic_score(query, retrieved, ground_truth)\n",
        "    kw = keyword_score(retrieved, keywords)\n",
        "\n",
        "    sem_norm = max(0, min((sem + 1) / 2, 1))  # normalize cosine similarity\n",
        "\n",
        "    score = (sem_norm * 0.8) + (kw * 0.2)\n",
        "    return round(score * 100, 2)   # return as 0‚Äì100 score\n",
        "\n",
        "\n",
        "\n",
        "# Example evaluation set (I will expand later as needed)\n",
        "\n",
        "EVAL_SET = [\n",
        "    {\n",
        "        \"query\": \"ŸÖÿß ŸáŸä ÿ¥ÿ±Ÿàÿ∑ ÿßŸÑÿ™ÿÆÿ±ÿ¨ÿü\",\n",
        "        \"keywords\": [\"ÿßŸÑŸÖÿπÿØŸÑ\", \"ÿßŸÑÿ™ÿÆÿ±ÿ¨\"],\n",
        "        \"ground_truth\": \"Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸÖÿπÿØŸÑ ÿßŸÑÿ™ÿ±ÿßŸÉŸÖŸä ŸÑÿß ŸäŸÇŸÑ ÿπŸÜ ÿßŸÑŸÖŸÇÿ®ŸàŸÑ ‚Ä¶\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"ŸÖÿ™Ÿâ Ÿäÿ≠ÿ±ŸÖ ÿßŸÑÿ∑ÿßŸÑÿ® ŸÖŸÜ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ± ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿü\",\n",
        "        \"keywords\": [\"75\", \"ÿ≠ÿ±ŸÖÿßŸÜ\"],\n",
        "        \"ground_truth\": \"ŸäŸèÿ≠ÿ±ŸÖ ÿßŸÑÿ∑ÿßŸÑÿ® ÿ•ÿ∞ÿß ÿ™ÿ¨ÿßŸàÿ≤ ÿ∫Ÿäÿßÿ®Ÿá 25Ÿ™ ‚Ä¶\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Main evaluation loop\n",
        "\n",
        "results = []\n",
        "\n",
        "for item in tqdm(EVAL_SET):\n",
        "    q = item[\"query\"]\n",
        "    kw = item[\"keywords\"]\n",
        "    gt = item[\"ground_truth\"]\n",
        "\n",
        "    retrieved = retrieve(q, k=3)\n",
        "    docs = retrieved[\"documents\"]\n",
        "    top_text = docs[0][0] if isinstance(docs[0], list) else docs[0]\n",
        "\n",
        "    score = hybrid_score(q, top_text, kw, gt)\n",
        "\n",
        "    results.append({\n",
        "        \"query\": q,\n",
        "        \"retrieved\": top_text,\n",
        "        \"score\": score\n",
        "    })\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"retrieval_eval_results.csv\", index=False)\n",
        "\n",
        "df\n"
      ]
    }
  ]
}